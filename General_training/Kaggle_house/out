Iteration 1: train loss 0.08793314546346664, test loss 0.1990697830915451
Iteration 2: train loss 0.07877983152866364, test loss 0.1681610643863678
Iteration 3: train loss 0.07580529153347015, test loss 0.23149657249450684
Iteration 4: train loss 0.08446221798658371, test loss 0.19885806739330292
Iteration 5: train loss 0.07255157083272934, test loss 0.23608548939228058
Iteration 6: train loss 0.09363913536071777, test loss 0.22058343887329102
Iteration 7: train loss 0.07659227401018143, test loss 0.20463763177394867
Iteration 8: train loss 0.08121775835752487, test loss 0.267806738615036
Iteration 9: train loss 0.0748305693268776, test loss 0.22301870584487915
Iteration 10: train loss 0.09900110214948654, test loss 0.20474466681480408
Iteration 11: train loss 0.0727950930595398, test loss 0.1904720813035965
Iteration 12: train loss 0.0854920744895935, test loss 0.2537577152252197
Iteration 13: train loss 0.0900271013379097, test loss 0.18634209036827087
Iteration 14: train loss 0.09409298747777939, test loss 0.2000599503517151
Iteration 15: train loss 0.08478192239999771, test loss 0.19522738456726074
Iteration 16: train loss 0.07967542111873627, test loss 0.18121658265590668
Iteration 17: train loss 0.08031075447797775, test loss 0.21889716386795044
Iteration 18: train loss 0.07436475157737732, test loss 0.18424269556999207
Iteration 19: train loss 0.0806710496544838, test loss 0.19848677515983582
Iteration 20: train loss 0.08294471353292465, test loss 0.20218175649642944
Iteration 21: train loss 0.07758928090333939, test loss 0.20108570158481598
Iteration 22: train loss 0.08196257799863815, test loss 0.2050008773803711
Iteration 23: train loss 0.08514605462551117, test loss 0.2130010724067688
Iteration 24: train loss 0.08372236788272858, test loss 0.203746497631073
Iteration 25: train loss 0.09185374528169632, test loss 0.1964472532272339
Iteration 26: train loss 0.08612358570098877, test loss 0.19418202340602875
Iteration 27: train loss 0.07387599349021912, test loss 0.2227647751569748
Iteration 28: train loss 0.0803210586309433, test loss 0.20941896736621857
Iteration 29: train loss 0.08019426465034485, test loss 0.17766419053077698
Iteration 30: train loss 0.07699064910411835, test loss 0.25391581654548645
Iteration 31: train loss 0.07530388981103897, test loss 0.21219410002231598
Iteration 32: train loss 0.08352120965719223, test loss 0.19187496602535248
Iteration 33: train loss 0.07917354255914688, test loss 0.2388598620891571
Iteration 34: train loss 0.08256234228610992, test loss 0.21390219032764435
Iteration 35: train loss 0.07715127617120743, test loss 0.1995490938425064
Iteration 36: train loss 0.06768511235713959, test loss 0.1990063190460205
Iteration 37: train loss 0.08055498450994492, test loss 0.1998036652803421
Iteration 38: train loss 0.08088681846857071, test loss 0.20380043983459473
Iteration 39: train loss 0.10047221183776855, test loss 0.20593643188476562
Iteration 40: train loss 0.07149054110050201, test loss 0.22629794478416443
Iteration 41: train loss 0.07220496982336044, test loss 0.21211442351341248
Iteration 42: train loss 0.08890990167856216, test loss 0.2037944346666336
Iteration 43: train loss 0.07791374623775482, test loss 0.23104365170001984
Iteration 44: train loss 0.07980384677648544, test loss 0.2009502351284027
Iteration 45: train loss 0.08131776005029678, test loss 0.21448005735874176
Iteration 46: train loss 0.08234161138534546, test loss 0.1806240975856781
Iteration 47: train loss 0.07705497741699219, test loss 0.22546859085559845
Iteration 48: train loss 0.089032381772995, test loss 0.16870352625846863
Iteration 49: train loss 0.07964882999658585, test loss 0.22781610488891602
Iteration 50: train loss 0.07469119131565094, test loss 0.24821899831295013
Iteration 51: train loss 0.09204015880823135, test loss 0.18648168444633484
Iteration 52: train loss 0.07710323482751846, test loss 0.20209075510501862
Iteration 53: train loss 0.08392948657274246, test loss 0.20723044872283936
Iteration 54: train loss 0.08488886058330536, test loss 0.2129792720079422
Iteration 55: train loss 0.07753194123506546, test loss 0.206148162484169
Iteration 56: train loss 0.07372824847698212, test loss 0.1884414106607437
Iteration 57: train loss 0.08358222246170044, test loss 0.2077590823173523
Iteration 58: train loss 0.07141750305891037, test loss 0.24961790442466736
Iteration 59: train loss 0.08513138443231583, test loss 0.2039736658334732
Iteration 60: train loss 0.08013394474983215, test loss 0.21256358921527863
Iteration 61: train loss 0.08641309291124344, test loss 0.21510857343673706
Iteration 62: train loss 0.08818092197179794, test loss 0.2169351726770401
Iteration 63: train loss 0.10184520483016968, test loss 0.19748146831989288
Iteration 64: train loss 0.07686900347471237, test loss 0.3169831335544586
Iteration 65: train loss 0.08062487095594406, test loss 0.22870613634586334
Iteration 66: train loss 0.0910339206457138, test loss 0.21292747557163239
Iteration 67: train loss 0.08465207368135452, test loss 0.2113969773054123
Iteration 68: train loss 0.08167616277933121, test loss 0.17844974994659424
Iteration 69: train loss 0.07075197994709015, test loss 0.21945065259933472
Iteration 70: train loss 0.07518168538808823, test loss 0.22342810034751892
Iteration 71: train loss 0.09047244489192963, test loss 0.20689572393894196
Iteration 72: train loss 0.07919459044933319, test loss 0.2848724126815796
Iteration 73: train loss 0.07645217329263687, test loss 0.194541335105896
Iteration 74: train loss 0.07074616104364395, test loss 0.2061992585659027
Iteration 75: train loss 0.0682629719376564, test loss 0.1925499588251114
Iteration 76: train loss 0.07968974858522415, test loss 0.1882377713918686
Iteration 77: train loss 0.07435498386621475, test loss 0.2247263491153717
Iteration 78: train loss 0.08378592133522034, test loss 0.25068268179893494
Iteration 79: train loss 0.07750459760427475, test loss 0.24117548763751984
Iteration 80: train loss 0.07973030209541321, test loss 0.19581569731235504
Iteration 81: train loss 0.07777490466833115, test loss 0.19582822918891907
Iteration 82: train loss 0.07250610738992691, test loss 0.21278072893619537
Iteration 83: train loss 0.07134648412466049, test loss 0.23126466572284698
Iteration 84: train loss 0.07889335602521896, test loss 0.21861274540424347
Iteration 85: train loss 0.07398013770580292, test loss 0.21771453320980072
Iteration 86: train loss 0.08145734667778015, test loss 0.23755285143852234
Iteration 87: train loss 0.07812172174453735, test loss 0.20504963397979736
Iteration 88: train loss 0.08246731013059616, test loss 0.2552608847618103
Iteration 89: train loss 0.07506636530160904, test loss 0.21394290030002594
Iteration 90: train loss 0.08191855996847153, test loss 0.22668547928333282
Iteration 91: train loss 0.08545482158660889, test loss 0.22374974191188812
Iteration 92: train loss 0.07894982397556305, test loss 0.24863457679748535
Iteration 93: train loss 0.08582931011915207, test loss 0.2061774581670761
Iteration 94: train loss 0.07644648104906082, test loss 0.2371022254228592
Iteration 95: train loss 0.08241879194974899, test loss 0.2539941072463989
Iteration 96: train loss 0.08072204142808914, test loss 0.20999981462955475
Iteration 97: train loss 0.08013539016246796, test loss 0.210703045129776
Iteration 98: train loss 0.07184877246618271, test loss 0.22038652002811432
Iteration 99: train loss 0.08628056198358536, test loss 0.1727602779865265
Iteration 100: train loss 0.09209605306386948, test loss 0.2963273227214813
0.08090093627572059
0.21425390496850014
